## STEPS TO CREATE GENERAL TASK TO CREATE REPOST TABLE FOR MYFOLIO

### 1. Manual check of Gateway

Gateway has been deployed on the test server with the domain `nimbus2-data-pipeline.appspot.com` \
However it is in the process of being completed. Maybe it doesn't have the data we need. We need to make sure we can get the data we need from Gateway. To do this, use a simple HTTP request from console. Of course you can use support tools like `Postman` to make http requests

- Creating a task for fetching data

```bash
# Use http post request
curl -X 'POST' 'https://nimbus2-data-pipeline.appspot.com/api/jobs/reports' \
-H 'accept: application/json' \
-H 'Content-Type: application/json' \
-H 'Authorization: Token <YOUR_ACCESS_TOKEN>' \
-d '{"date_from": "2023-07-25","date_to": "2023-07-26","publisher": "GMB","report_name": "LOCATION_INSIGHTS_V2"}'

# This is just an example with publisher GMB. Feel free to change it according to your needs
# If the post command succeeds and data is returned, pay attention to the id field
{
    ...
    "id": <JOB_ID>,
    ...
}
```

- Getting a result with the id you got

```bash
# Use http get request
curl -L -X 'GET' "https://nimbus2-data-pipeline.appspot.com/api/jobs/reports/<JOB_ID>" \
-H 'Authorization: Token <YOUR_ACCESS_TOKEN>'

# Pay attention to the completed field
{
    ...
    "id": <JOB_ID>,
    "completed": true,
    ...
}
# If the completed field is true, , request has been completed. Please make the next request. If it is false, the request is still incomplete. Please wait a little longer because the data generated for each request is quite a lot

# Make this request to get the data fields returned by the gateway
curl -L -X 'GET' "https://nimbus2-data-pipeline.appspot.com/api/jobs/reports/<JOB_ID>/results" \
-H 'Authorization: Token <YOUR_ACCESS_TOKEN>'
```

If the response contain the fields you need, then you can skip [Setup gateway locally](#2-setup-gateway-locally) step and start developing. If not, you must do that step

### 2. Setup gateway locally

- Step 1 : Setup gateway in local according to the attached instructions\
  https://github.com/leadplusjapan/gateway

- Step 2 : Do [manual check of Gateway](#1-manual-check-of-gateway) again with the following changes
  - Run local gateway, access django admin and generate local access-token
  - Change `nimbus2-data-pipeline.appspot.com` to `127.0.0.1:8000`
  - Use the token you just created to make the request

```bash
# HTTP post request like
curl -X 'POST' 'https://127.0.0.1:8000/api/jobs/reports/' \
-H 'accept: application/json' \
-H 'Content-Type: application/json' \
-H 'Authorization: Token <YOUR_LOCAL_ACCESS_TOKEN>' \
-d '{"date_from": "2023-07-25","date_to": "2023-07-26","publisher": "GMB","report_name": "LOCATION_INSIGHTS_V2"}'
```

It may still not have the fields you need. If so, contact the instructor, they will give you a different version of the library in the requirement. Please uninstall old version libraries and install new version libraries. Make the request again. Then you will have the data fields you need

### 3. Connect Airflow to Gateway locally

- Step 1 : Access Airflow connection
  - Run Airflow and login Airflow home in here \
    http://localhost:8080/home
  - In menu, select <kbd>Admin</kbd> => <kbd>Connection</kbd> => <kbd>Edit</kbd> at the record where the `Connd Id` is `http_gateway`
- Step 2 : Edit airflow connection

  When accessing `Edit Connection` page, fill in the properties as follows :

  - In `Connection Id`, `Connection Type` properties, keep the same
  - In the `Description` properties, don't need to fill in anything
  - `Login` and `Password` properties must be left blank. Maybe the system will automatically enter username and password in these 2 fields if you choose to save the password. Please delete it
  - In the remaining properties

    <table>
      <tr>
          <td> <strong> Properties </strong> </td>
          <td> <strong> Value (connect to local server) </strong> </td>
          <td> <strong> Value (connect to remote server) </strong> </td>
      </tr>
      <tr>
          <td> Extra </td>
        <td>
        
    ```json
    { "Authorization": "Access-Token <YOUR_LOCAL_ACCESS_TOKEN>" }
    ```
      </td>
        <td>
        
    ```json
    { "Authorization": "Access-Token <YOUR_REMOTE_ACCESS_TOKEN>" }
    ```
      </td>
      </tr>
      <tr>
          <td> Host </td>
          <td>
            host.docker.internal
          </td>
          <td>
            nimbus2-data-pipeline.appspot.com
          </td>
      </tr>
      <tr>
          <td> Schema </td>
          <td>
            http
          </td>
          <td>
            https
          </td>
      </tr>
      <tr>
          <td> Port </td>
          <td>
            8000
          </td>
          <td>
            -
          </td>

      </tr>
    </table>

- Step 3 : Checking connection
  - Make sure the gateway is running. Local Gateway will run at port 8000 by default
  - Click the <kbd>Test</kbd> button, if the connection is successful, you will get the message `Connection successfully tested`. If not, an error message will appear. Please read it and check the previous steps again
  - Click the <kbd>Save</kbd> button after successful connection

### 4. Update Airflow and Bigquery

Based on data obtained from get request, do the following steps :

- Modify the respective classes and queries \
  Depending on specific requirements to focus on the object to be modify
- In Bigquery, modify the tables to match the new classes and queries \
  To determine which tables to modify, notice the queries in the .sql files

Pay attention to the following :

- Instead of modify, you can create a new one with a new name and keep the old one
- Inside classes and queries, when adding new fields from the updated gateway version, delete old fields that are no longer available
- Subclass inherit from the superclass. So find the relation between them to modify all that is necessary
- File [core/dag_builder.py](https://github.com/leadplusjapan/nimbus-composer/blob/develop/dags/core/dag_builder.py) is the superclass of all dag classes
- Each dag contains multiple tasks and group of tasks. You can show graph on Airflow Web UI for easier viewing
- Each query also queries many different tables. Modify the table name if you create a new table
- Query, dataset and table in Bigquery must follow the [Bigquery Convention](https://reachlocaljapan.atlassian.net/wiki/spaces/PT/pages/917831706/BigQuery+Conventions). Respective classes should also follow that

### 5. Check results

After doing all the above steps, run the pipeline you modified and check the result.

If the pipeline runs successfully and the data has been saved to Bigquery then you are successful.

If an error occurs during the process, click the <kbd>Log</kbd> button to view the error information. Please read the error and fix it.

You can see the rendered SQL query in respective Task by clicking the <kbd>Rendered Template</kbd> button. This is good debugging functionality
